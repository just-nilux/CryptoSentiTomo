raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
end_epoch <- '1509494400' #1st Nov 00:00
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
View(old.df)
max_epoch <- max(old.df$created_utc)
end_epoch <- as.numeric('1509494400') #1st Nov 00:00
new.df <- crawl_reddit_pushshift(max_epoch)
View(new.df)
final.df <- rbind(old.df,new.df)
final <- rbind(old.df,new.df)
View(old.df)
colnames(new.df)
colnames(old.df)
sort(colnames(old.df))
sort(colnames(new.df))
View(old.df)
View(new.df)
new_colnames <- sort(colnames(new.df))
colnames(new.df) <- new_colnames
colnames(old.df) <- new_colnames
final.df <- rbind(old.df,new.df)
View(final.df)
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
rm(list = ls())
packages <- c("jsonlite","httr", # support API call
"lubridate")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&after=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
end_epoch <- as.numeric('1509494400') #1st Nov 00:00
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
# delete obsolete column "X"
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- max(old.df$created_utc)
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
# Re-order colnames for unity
new_colnames <- sort(colnames(new.df))
colnames(new.df) <- new_colnames
colnames(old.df) <- new_colnames
final.df <- rbind(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',max_epoch,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
# delete obsolete column "X"
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- max(old.df$created_utc)
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
# Re-order colnames for unity
new_colnames <- sort(colnames(new.df))
colnames(new.df) <- new_colnames
colnames(old.df) <- new_colnames
final.df <- rbind(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',max_epoch,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
View(old.df)
View(new.df)
old.df <- old.df[ , -which(names(old.df) %in% new_colnames)]
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , which(names(old.df) %in% new_colnames)]
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
# extract latest epoch time from old dataset
max_epoch <- max(old.df$created_utc)
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
# Re-order colnames for unity
new_colnames <- sort(colnames(new.df))
colnames(new.df) <- new_colnames
old.df <- old.df[ , which(names(old.df) %in% new_colnames)]
colnames(old.df) <- new_colnames
final.df <- rbind(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',max_epoch,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
View(old.df)
View(new.df)
View(old.df)
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&after=1506816000&stickied=false&locked=false&is_video=false&size=500"
# Extract Reddit submissions using filters
# - Subreddit = cryptocurrency
# - After = 1st Oct 2017 (epoch time)
# - No sticked, locked and video submissions
# - Size = 500 (maximum per call)
raw_data <- GET(url = url,
path = path,
query = query)
names(raw_data)
raw_data$status_code #if status = 200 -> fine
test <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(test)
class(extract.data)
# convert to dataframe
reddit.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
reddit.df <- reddit.df[ , -which(names(reddit.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
# save genesis dataset
write.csv(reddit.df,paste0(report_path,'Crypto_Reddit.csv'))
View(reddit.df)
reddit.df <- reddit.df[,new_colnames]
test <- reddit.df[ , which(names(reddit.df) %in% new_colnames)]
View(test)
rm(test)
rm(list = ls())
end_epoch <- as.numeric('1509494400') #1st Nov 00:00
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&after=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
View(old.df)
max_epoch <- max(old.df$created_utc)
new.df <- crawl_reddit_pushshift(max_epoch)
View(new.df)
new.df <- new.df[ , -which(names(new.df) %in% "X")]
new.df <- crawl_reddit_pushshift(max_epoch)
new.df <- new.df[ , -which(names(new.df) %in% c("X"))]
new.df <- crawl_reddit_pushshift(max_epoch)
View(old.df)
colnames(new.df)
sort(colnames(new.df))
sort(colnames(old.df))
old.df <- old.df[ , which(names(old.df) %in% c("X"))]
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
View(new.df)
View(old.df)
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
final.df <- rbind.fill(old.df,new.df)
packages <- c("jsonlite","httr", # support API call
"dplyr")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
final.df <- rbind.fill(old.df,new.df)
?rbind.fill
final.df <- bind_rows(old.df,new.df)
View(final.df)
warnings()
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',max_epoch,'.csv'))
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- max(old.df$created_utc)
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',max_epoch,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&after=1506816000&stickied=false&locked=false&is_video=false&size=500"
# Extract Reddit submissions using filters
# - Subreddit = cryptocurrency
# - After = 1st Oct 2017 (epoch time)
# - No sticked, locked and video submissions
# - Size = 500 (maximum per call)
raw_data <- GET(url = url,
path = path,
query = query)
names(raw_data)
raw_data$status_code #if status = 200 -> fine
test <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(test)
class(extract.data)
# convert to dataframe
reddit.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
reddit.df <- reddit.df[ , -which(names(reddit.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
# save genesis dataset
write.csv(reddit.df,paste0(report_path,'Crypto_Reddit.csv'))
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&after=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
end_epoch <- as.numeric('1509494400') #1st Nov 00:00
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- max(old.df$created_utc)
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',unix2POSIXct(max_epoch),'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
rm(list = ls())
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&after=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
end_epoch <- as.numeric('1509494400') #1st Nov 00:00
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
max_epoch <- max(old.df$created_utc)
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',unix2POSIXct(max_epoch),'.csv'))
unix2POSIXct <- function(time) structure(time, class = c("POSIXt"))
unix2POSIXct(1509494400)
unix2POSIXct('1509494400')
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
unix2POSIXct('1509494400')
substr(unix2POSIXct('1509494400'),0,11)
substr(as.character(unix2POSIXct('1509494400')),0,11)
substr(as.character(unix2POSIXct('1509494400')),0,10)
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- max(old.df$created_utc)
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
as.character(unix2POSIXct(max_epoch),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
max_epoch <- as.numeric(max(old.df$created_utc))
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.numeric(max(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
as.character(unix2POSIXct(max_epoch),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
as.character(unix2POSIXct(max_epoch),0,10)
max_epoch
as.character(unix2POSIXct(max_epoch),0,10)
unix2POSIXct('1509494400')
max_epoch
unix2POSIXct(as.character(max_epoch))
max_epoch <- as.character(max(old.df$created_utc))
end_epoch <- as.character('1509494400') #1st Nov 00:00
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.character(max(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
as.character(unix2POSIXct(max_epoch),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
unix2POSIXct(as.character(max_epoch))
end_epoch <- as.character('1509494400') #1st Nov 00:00
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.character(max(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
as.character(unix2POSIXct(as.character(max_epoch)),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
as.character(unix2POSIXct(as.character(max_epoch)),0,10)
unix2POSIXct(as.character(max_epoch))
end_epoch <- as.numeric('1509494400') #1st Nov 00:00
max_epoch <- as.numeric(max(old.df$created_utc))
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.numeric(max(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(as.character(max_epoch)),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
substr(unix2POSIXct(as.character(max_epoch)),0,10)
substr(unix2POSIXct(as.character(max_epoch)),0,10)
substr(unix2POSIXct(max_epoch),0,10)
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.numeric(max(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(max_epoch),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
View(final.df)
