report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.numeric(max(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(max_epoch),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
# ----------------------------------
reddit.df <- read.csv("./Crawlers/2b. Reddit Report/Crypto_Reddit.csv")
reddit.df <- read.csv("~/GitHub/NextBigCrypto-Senti/Crawlers/2b. Reddit Report/Crypto_Reddit.csv")
reddit.df <- reddit.df[ , -which(names(reddit.df) %in% c("X"))]
reddit.df <- unique(reddit.df)
View(new.df)
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&before=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
min_epoch <- as.numeric(min(old.df$created_utc))
end_epoch <- '1483228800' #1st Jan 2017
end_epoch <- as.numeric('1483228800') #1st Jan 2017
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&before=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
end_epoch <- as.numeric('1483228800') #1st Jan 2017
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(min_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(min_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch <= end_epoch){
break
}
}
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(min_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(min_epoch <= end_epoch){
break
}
}
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(min_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(min_epoch <= end_epoch){
break
}
}
min_epoch
end_epoch <- as.numeric('1483228800') #1st Jan 2017
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
rm(list = ls())
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&before=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
end_epoch <- as.numeric('1483228800') #1st Jan 2017
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(min_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(min_epoch <= end_epoch){
break
}
}
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
end_epoch <- as.numeric('1483228800') #1st Jan 2017
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(min_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(min_epoch <= end_epoch){
break
}
}
new.df <- crawl_reddit_pushshift(min_epoch)
min_epoch
a <- read.csv("2b. Reddit Report/Crypto_reddit.csv")
b <- read.csv("2b. Reddit Report/Crypto_reddit - Copy (2).csv")
b <- unique(b)
a <- unique(a)
c <- rbind(a,b)
c <- unique(c)
#Set up working directory
setwd("~/GitHub/NextBigCrypto-Senti/")
# Clear environment
rm(list = ls())
# Reddit data
reddit.df <- read.csv("~/GitHub/NextBigCrypto-Senti/Crawlers/2b. Reddit Report/Crypto_Reddit.csv")
reddit.df <- reddit.df[ , -which(names(reddit.df) %in% c("X"))]
reddit.df <- unique(reddit.df)
View(reddit.df)
reddit.df <- reddit.df[,colSums(is.na(reddit.df))<nrow(reddit.df)]
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
substr(unix2POSIXct(reddit.df$created_utc[1]),0,10)
unix2POSIXct(reddit.df$created_utc[1])
reddit.df$created_at <- unix2POSIXct(reddit.df$created_utc)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
reddit.df$author_flair_css_class <- trim(reddit.df$author_flair_css_class)
View(reddit.df)
colnames(reddit.df)
sort(colnames(reddit.df))
reddit.df$edited_at <- unix2POSIXct(reddit.df$edited)
sort(colnames(reddit.df))
processed.reddit <- reddit.df[,which(names(reddit.df)) %in% c('id','title','is_self','selftext','link_flair_css_class',
'link_flair_text','num_comments','score','domain',
'author','author_flair_css_class','author_flair_text',
'created_at','edited_at','full_link','url')]
processed.reddit <- reddit.df[,c('id','title','is_self','selftext','link_flair_css_class',
'link_flair_text','num_comments','score','domain',
'author','author_flair_css_class','author_flair_text',
'created_at','edited_at','full_link','url')]
View(processed.reddit)
dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_css_class),n())
dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text),n())
flair <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text),n())
View(flair)
flair <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text,link_flair_css_class),n())
processed.reddit$link_flair_css_class <- tolower(processed.reddit$link_flair_css_class)
processed.reddit$link_flair_text <- tolower(processed.reddit$link_flair_text)
flair <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text,link_flair_css_class),n())
flair_css <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_css_class),n())
View(flair_css)
flair_all <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text,link_flair_css_class),n())
flair_text <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text),n())
View(flair_text)
View(flair_all)
rm(list = ls())
#Set up working directory
setwd("~/GitHub/NextBigCrypto-Senti/")
# Clear environment
rm(list = ls())
# Convert epoch time
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
# Reddit data---------------------------------------
reddit.df <- read.csv("~/GitHub/NextBigCrypto-Senti/Crawlers/2b. Reddit Report/Crypto_Reddit.csv")
reddit.df <- reddit.df[ , -which(names(reddit.df) %in% c("X"))]
reddit.df <- unique(reddit.df)
# Clear all NULL columns
reddit.df <- reddit.df[,colSums(is.na(reddit.df))<nrow(reddit.df)]
# Add created_at column (human intepretation date time)
reddit.df$created_at <- unix2POSIXct(reddit.df$created_utc)
reddit.df$edited_at <- unix2POSIXct(reddit.df$edited)
# Preprocessing ------------------------------------
# Extract subset for EDA
processed.reddit <- reddit.df[,c('id','title','is_self','selftext','link_flair_css_class',
'link_flair_text','num_comments','score','domain',
'author','author_flair_css_class','author_flair_text',
'created_at','edited_at','full_link','url')]
# EDA
link_flair_all <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text,link_flair_css_class),n())
link_flair_css <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_css_class),n())
link_flair_text <- dplyr::summarize(dplyr::group_by(processed.reddit,link_flair_text),n())
# turn all flairs into lower case
processed.reddit$link_flair_css_class <- tolower(processed.reddit$link_flair_css_class)
processed.reddit$link_flair_text <- tolower(processed.reddit$link_flair_text)
# trip whitespace leading/trailing in author flairs
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
processed.reddit$author_flair_css_class <- trim(processed.reddit$author_flair_css_class)
author_flair_all <- dplyr::summarize(dplyr::group_by(processed.reddit,author_flair_text,author_flair_css_class),n())
author_flair_css <- dplyr::summarize(dplyr::group_by(processed.reddit,author_flair_css_class),n())
author_flair_text <- dplyr::summarize(dplyr::group_by(processed.reddit,author_flair_text),n())
View(author_flair_text)
View(author_flair_css)
View(author_flair_all)
View(author_flair_text)
rm(list = ls())
setwd("~/GitHub/NextBigCrypto-Senti/Crawlers")
# install packages if not available
packages <- c("jsonlite","httr", # support API call
"dplyr")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
#--------------------------------#
# GENESIS CODE (run only 1 time) #
#--------------------------------#
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptomarkets&before=1509647464&stickied=false&locked=false&is_video=false&size=500"
# Extract Reddit submissions using filters
# - Subreddit = cryptomarkets
# - From 1st Jan - Now (epoch time)
# - No sticked, locked and video submissions
# - Size = 500 (maximum per call)
raw_data <- GET(url = url,
path = path,
query = query)
names(raw_data)
raw_data$status_code #if status = 200 -> fine
test <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(test)
class(extract.data)
# convert to dataframe
reddit.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
reddit.df <- reddit.df[ , -which(names(reddit.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
# save genesis dataset
write.csv(reddit.df,paste0(report_path,'CryptoMarkets_Reddit.csv'))
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptomarkets&before=1509647464&stickied=false&locked=false&is_video=false&size=500"
# Extract Reddit submissions using filters
# - Subreddit = cryptomarkets
# - From 1st Jan - Now (epoch time)
# - No sticked, locked and video submissions
# - Size = 500 (maximum per call)
raw_data <- GET(url = url,
path = path,
query = query)
names(raw_data)
raw_data$status_code #if status = 200 -> fine
test <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(test)
class(extract.data)
# convert to dataframe
reddit.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
reddit.df <- reddit.df[ , -which(names(reddit.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
report_path <- '2b. Reddit Report/'
# save genesis dataset
write.csv(reddit.df,paste0(report_path,'CryptoMarkets_Reddit.csv'))
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&after=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&before=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&before=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
end_epoch <- as.numeric('1483228800') #1st Jan 00:00
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"CryptoMarkets_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
min_epoch <- as.numeric(min(old.df$created_utc))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(min_epoch)
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'CryptoMarkets_Reddit_',
substr(unix2POSIXct(min_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(min_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'CryptoMarkets_Reddit.csv'))
if(min_epoch <= end_epoch){
break
}
}
