Using Rfacebook package
source('F:/Researches and works/R/AbbVie/Companies.R')
source('F:/Researches and works/R/AbbVie/Companies.R')
token <- fb_oauth
me <- getUsers("me", token = fb_oauth)
me
searchGroup("AbbVie",token)
getGroup(278782302258949, token)
searchGroup("AbbVie",token)
searchFacebook("AbbVie",token)
getNewsfeed(token, n= 10)
callAPI("https://graph.facebook.com/v2.0/barackobama?fields=id", token)
getGroup(278782302258949, token)
AbbVie_main <- getGroup(278782302258949, token, n = 100)
AbbVie_main
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
# visualize evolution in metric
library(ggplot2)
library(scales)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
library(ggplot2)
library(scales)
install.packages("ggplot2")
install.packages("scales")
library(ggplot2)
library(scales)
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
AbbVie_page <- getPage("AbbVie",token, n = 5000)
AbbVie_page <- getPage("AbbVieGlobal",token, n = 5000)
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
page <- getPage("humansofnewyork", token, n = 5000)
page[which.max(page$likes_count), ]
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
page <- getPage("AbbVieGlobal", token, n = 5000)
page[which.max(page$likes_count), ]
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100)) + theme_bw() + theme(axis.title.x = element_blank())
AbbVie_group <- getGroup(278782302258949, token, n = 5000)
AbbvieGlobal <- getPage("AbbVieGlobal", token, n = 5000)
searchFacebook(string ="yahoo",token, n=100)
?searchFacebook
getNewsfeed(token,n=10)
getNewsfeed(token,n=10)
searchFacebook("AbbVie",token)
saigonfirewok <- getPage("SaigonFirewok",n=100)
saigonfirewok <- getPage("SaigonFirewok",token, n=100)
searchPages("SaigonFirewok",token)
getInsights(1710766875823835,token)
getInsights(1710766875823835,token,page_impression)
getInsights(1710766875823835,token,page_impressions)
getInsights(1710766875823835,token, metric ='page_impressions')
getInsights(1506617922967149,token, metric ='page_impressions')
require (Rfacebook)
library(devtools)
# visualize evolution in metric
library(ggplot2)
library(scales)
#Table manipulation
library(dplyr)
#Get FB_Oauth
fb_oauth <- fbOAuth(app_id="204227866723896",
app_secret="e39f8a7750fd165276e0d36709201f92",
extended_permissions = TRUE)
x <- fb_oauth
searchGroup("AbbVie",x)
groupFb<- searchGroup("AbbVie",x)
View(groupFb)
AbbVie_group <- getGroup(278782302258949, x, n = 5000)
groupFb<- searchGroup("AbbVieGlobal",x)
groupFb<- searchGroup("AbbVie",x)
View(groupFb)
date_diff <- as.Date(as.character("2016/04/14"), format="%Y/%m/%d")-
as.Date(as.character("2016/01/01"), format="%Y/%m/%d")
date_diff
as.Date(as.character("2016/12/31"), format="%Y/%m/%d")-
as.Date(as.character("2016/10/01"), format="%Y/%m/%d")
as.Date(as.character("2015/12/31"), format="%Y/%m/%d")-
as.Date(as.character("2015/10/15"), format="%Y/%m/%d")
# install packages if not available
packages <- c('twitteR', 'tesseract', '	abbyyR','data.table','dplyr')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR()
# Set up working directory
setwd("C:/Users/BluePhoenix/Documents/GitHub/NextBigCrypto-Senti/Crawlers")
source("1.TW_Functions.R")
source("../1. Twitter/1.TW_Functions.R")
source("/1. Twitter/1.TW_Functions.R")
source("C:/Users/BluePhoenix/Documents/GitHub/NextBigCrypto-Senti/Crawlers/1. Twitter/1.TW_Functions.R")
coins_list <- read.csv("Top50_Oct7.csv")
source("//1. Twitter/1.TW_Functions.R")
source("./1. Twitter/1.TW_Functions.R")
coins_list <- read.csv("Top50_Oct7.csv")
View(coins_list)
coins_list$X[1]
i <- 1
ticker <- "$BTC"
print(i,ticker)
print(i +ticker)
print(i + ticker)
print(paste(i,ticker))
test <- read.csv("./1b. Report/1_$BTC_2017-10-07.csv")
View(test)
test <- read.csv("./1b. Report/1_$BTC_2017-10-07.csv",sep = ',')
test <- test[, which(names(test) %!in% c("X"))]
test <- test[, !which(names(test) %in% c("X"))]
test <- read.csv("./1b. Report/1_$BTC_2017-10-07.csv")
test <- test[, -which(names(test) %in% c("X"))]
options("scipen"=100, "digits"=4)
View(test)
max(test$status_id
)
?search_tweets
ticker <- coins_list$X[1]
i <- coins_list$X[1]
ticker <- coins_list$ticker[1]
coins_list$ticker[1]
ticker <- coins_list$ticker[1]
i <- coins_list$X[1]
olddf <- paste0('./1b. Report/',i,'_',ticker,'_','2017-10-07','.csv')
# Read the old data file
old_df_path <- paste0('./1b. Report/',i,'_',ticker,'_','2017-10-07','.csv')
old_df <- read.csv(old_df_path)
old_df <- old_df[, -which(names(old_df) %in% c("X"))]
since <- max(old_df$status_id)
path = '1b. Report/'
# Read the old data file
old_df_path <- paste0('./1b. Report/',i,'_',ticker,'_','2017-10-07','.csv')
old_df <- read.csv(old_df_path)
old_df <- old_df[, -which(names(old_df) %in% c("X"))]
since <- max(old_df$status_id)
df <- search_tweets(ticker,
n,
type = 'recent',
lang = 'en',
since = since,
include_rts = FALSE,
retryonratelimit = TRUE)
print(paste(i,ticker))
write.csv(df,paste0(path,i,'_',ticker,'_',Sys.Date(),'.csv'))
path = '1b. Report/'
# Read the old data file
old_df_path <- paste0('./1b. Report/',i,'_',ticker,'_','2017-10-07','.csv')
old_df <- read.csv(old_df_path)
old_df <- old_df[, -which(names(old_df) %in% c("X"))]
since <- max(old_df$status_id)
df <- search_tweets(ticker,
500000,
type = 'recent',
lang = 'en',
since = since,
include_rts = FALSE,
retryonratelimit = TRUE)
print(paste(i,ticker))
write.csv(df,paste0(path,i,'_',ticker,'_',Sys.Date(),'.csv'))
df <- search_tweets(ticker,
500,
type = 'recent',
lang = 'en',
since = since,
include_rts = FALSE,
retryonratelimit = TRUE)
ticker <- as.character(coins_list$ticker[1])
df <- search_tweets(ticker,
500,
type = 'recent',
lang = 'en',
since = since,
include_rts = FALSE,
retryonratelimit = TRUE)
View(df)
df <- search_tweets(ticker,
500,
type = 'recent',
lang = 'en',
since_id = since,
include_rts = FALSE,
retryonratelimit = TRUE)
View(df)
# Function to crawl tweets for top 50 tickers
get_tweets <- function(i,ticker,n){
path = '1b. Report/'
# Read the old data file
old_df_path <- paste0('./1b. Report/',i,'_',ticker,'_','2017-10-07','.csv')
old_df <- read.csv(old_df_path)
old_df <- old_df[, -which(names(old_df) %in% c("X"))]
since <- max(old_df$status_id)
df <- search_tweets(ticker,
n,
type = 'recent',
lang = 'en',
since_id = since,
include_rts = FALSE,
retryonratelimit = TRUE)
print(paste(i,ticker))
write.csv(df,paste0(path,i,'_',ticker,'_',Sys.Date(),'.csv'))
}
for (i in 1:nrow(coins_list)){
get_tweets(coins_list$X[i],as.character(coins_list$ticker[i]),100000)
}
for (i in 2:nrow(coins_list)){
get_tweets(coins_list$X[i],as.character(coins_list$ticker[i]),100000)
}
#---------------------------------------------------------------
# Function to swap Twitter Auth between 4 keys when entry rate runs out
swap_auth <- function(x){
if (x == 1){
# 1
consumer_key <- 'cdi1LlwgzdXzR4Wxz8T3Gude6'
consumer_secret <- 's3hpLYXs9ULY1YwzyTRP8aRovp3rvkjUM9ue9usi8MotrvUgOG'
access_token <- '240771509-MQiqGMegj3B4ohmRSi7mThfprMg7j9lAYkDB3s9W'
access_token_secret <- 'YZGMyJ6Jz3Ncx1SG59QXJGaRrEkYjvCTC71KPsFH2eaIi'
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
if (x == 2){
# 2
consumer_key <- 'yO6HMXQfaAazZfdyOQpPicX0M'
consumer_secret <- 'wT1lq9bd7WWJjoVw3aHKfdHbpdjxd8r8RKc56fGiQPGRaJgILP'
access_token <- '379008223-8gPeX8OJ5wxjILXYUMxKwTSOH30UJbYdUWNqCE53'
access_token_secret <- 'P3anD6dTrrQb6RUP4Me6HAMpgY8RU9QuORCrGI14f1Wis'
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
if (x == 3){
# 3
consumer_key="hNgXlxwQYcL71SxCddwvpTEVf"
consumer_secret="ZSXtL7Yq5QwkAvyCnm9hACaC6CosyHUOOnewv2ufL6IG8tQBCU"
access_token="838380485843763200-pAQXVTl89Dn1Pz2GnQzOacBmJnXPZz6"
access_token_secret="MtqyBbhUxM0zOTJIuRXUWtZMRmVnnjfFT0rs5X4odItdq"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
if (x == 4){
# 4
consumer_key <- "zow0fQ6Lv0j79gx4lDhBKVUDu"
consumer_secret = "fp33fr0VBkIIoPzpwgbCPemkZJ1E718TFqb8b86DKd0nVgGFEs"
access_token = "836598863582617600-Tjmc0MqCtcOZVjx9dto5wSBkdRgxDmh"
access_token_secret = "57THIHAlttLUf3y8x1P5U2JnQmcDIfDqq8xmZrwgD5Qo6"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
}
# Function to crawl tweets for top 50 tickers
get_tweets <- function(i,ticker,n,TW_key){
path = '1b. Report/'
# Rotate TW_key for each crypto ticker
swap_auth(TW_key)
# Read the old data file
old_df_path <- paste0('./1b. Report/',i,'_',ticker,'_','2017-10-07','.csv')
old_df <- read.csv(old_df_path)
old_df <- old_df[, -which(names(old_df) %in% c("X"))]
since <- max(old_df$status_id)
df <- search_tweets(ticker,
n,
type = 'recent',
lang = 'en',
since_id = since,
include_rts = FALSE,
retryonratelimit = TRUE)
print(paste(i,ticker))
write.csv(df,paste0(path,i,'_',ticker,'_',Sys.Date(),'.csv'))
}
# assign Twitter Auth key before crawling
TW_key <- 1
for (i in 5:nrow(coins_list)){
get_tweets(coins_list$X[i],as.character(coins_list$ticker[i]),100000, TW_key)
TW_key <- TW_key +1
if (TW_key == 5){TW_key <- 1}
}
test <- read.csv("1_$BTC_2017-10-13.csv")
test <- read.csv("./1b. Report/1_$BTC_2017-10-13.csv")
test0 <-read.csv("./1b. Report/1_$BTC_2017-10-07.csv")
final <- rbind(test0,test)
final <- unique(final)
write.csv(final,'./1b. Report/1_$BTC_FULL.csv')
options("scipen"=100, "digits"=4)
# Set up working directory
setwd("C:/Users/BluePhoenix/Documents/GitHub/NextBigCrypto-Senti/Crawlers")
# Clear environment
rm(list = ls())
# 1. Get coin list --------------------------------------------
# Obtain list of coins --> extract tickers + coin names
# devtools::install_github("amrrs/coinmarketcapr")
library(coinmarketcapr)
# latest_marketcap <- get_marketcap_ticker_all('EUR')
#
# coins_list <- latest_marketcap[,which(names(latest_marketcap) %in% c("name","symbol"))]
# coins_list$ticker <- paste0('$',coins_list$symbol)s
#
# # Take only top 50 (Oct 7)
# coins_list <- coins_list[1:50,]
# write.csv(coins_list,"Top50_Oct7.csv")
coins_list <- read.csv("Top50_Oct7.csv")
#---------------------------------------------------------------
# Function to swap Twitter Auth between 4 keys when entry rate runs out
swap_auth <- function(x){
if (x == 1){
# 1
consumer_key <- 'cdi1LlwgzdXzR4Wxz8T3Gude6'
consumer_secret <- 's3hpLYXs9ULY1YwzyTRP8aRovp3rvkjUM9ue9usi8MotrvUgOG'
access_token <- '240771509-MQiqGMegj3B4ohmRSi7mThfprMg7j9lAYkDB3s9W'
access_token_secret <- 'YZGMyJ6Jz3Ncx1SG59QXJGaRrEkYjvCTC71KPsFH2eaIi'
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
if (x == 2){
# 2
consumer_key <- 'yO6HMXQfaAazZfdyOQpPicX0M'
consumer_secret <- 'wT1lq9bd7WWJjoVw3aHKfdHbpdjxd8r8RKc56fGiQPGRaJgILP'
access_token <- '379008223-8gPeX8OJ5wxjILXYUMxKwTSOH30UJbYdUWNqCE53'
access_token_secret <- 'P3anD6dTrrQb6RUP4Me6HAMpgY8RU9QuORCrGI14f1Wis'
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
if (x == 3){
# 3
consumer_key="hNgXlxwQYcL71SxCddwvpTEVf"
consumer_secret="ZSXtL7Yq5QwkAvyCnm9hACaC6CosyHUOOnewv2ufL6IG8tQBCU"
access_token="838380485843763200-pAQXVTl89Dn1Pz2GnQzOacBmJnXPZz6"
access_token_secret="MtqyBbhUxM0zOTJIuRXUWtZMRmVnnjfFT0rs5X4odItdq"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
if (x == 4){
# 4
consumer_key <- "zow0fQ6Lv0j79gx4lDhBKVUDu"
consumer_secret = "fp33fr0VBkIIoPzpwgbCPemkZJ1E718TFqb8b86DKd0nVgGFEs"
access_token = "836598863582617600-Tjmc0MqCtcOZVjx9dto5wSBkdRgxDmh"
access_token_secret = "57THIHAlttLUf3y8x1P5U2JnQmcDIfDqq8xmZrwgD5Qo6"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
}
}
# Function to crawl tweets for top 50 tickers
get_tweets <- function(i,ticker,n,TW_key){
path = '1b. Report/'
path_weekly = '1b. Report/Weekly/'
# Rotate TW_key for each crypto ticker
swap_auth(TW_key)
# Read the old data file
old_df_path <- paste0(path_weekly,i,'_',ticker,'_','2017-10-07','.csv')
old_df <- read.csv(old_df_path)
#old_df <- old_df[, -which(names(old_df) %in% c("X"))]
since <- max(old_df$status_id)
df <- search_tweets(ticker,
n,
type = 'recent',
lang = 'en',
since_id = since,
include_rts = FALSE,
retryonratelimit = TRUE)
# Merge with old file
finaldf <- rbind(old_df,df)
# Print out current ticker
print(paste(i,ticker))
# Write finaldf into newest csv file
write.csv(finaldf,paste0(path,i,'_',ticker,'_FULL.csv'))
# Save update weekly
write.csv(df,paste0(path_weekly,i,'_',ticker,'_',Sys.Date(),'.csv'))
}
# assign Twitter Auth key before crawling
TW_key <- 1
for (i in 1:nrow(coins_list)){
get_tweets(coins_list$X[i],as.character(coins_list$ticker[i]),100000, TW_key)
TW_key <- TW_key +1
if (TW_key == 5){TW_key <- 1}
}
test <- read.csv('1b. Report/Weekly/1_$BTC_2017-10-07.csv)
test <- read.csv('1b. Report/Weekly/1_$BTC_2017-10-07.csv')
test <- test[, which(names(test) %!in% c("X"))]
test <- test[, -which(names(test) %in% c("X"))]
# Function to crawl tweets for top 50 tickers
get_tweets <- function(i,ticker,n,TW_key){
path = '1b. Report/'
path_weekly = '1b. Report/Weekly/'
# Rotate TW_key for each crypto ticker
swap_auth(TW_key)
# Read the old data file
old_df_path <- paste0(path_weekly,i,'_',ticker,'_','2017-10-07','.csv')
old_df <- read.csv(old_df_path)
# Delete "X" column (error when saving files)
old_df<- old_df[, -which(names(old_df) %in% c("X"))]
#old_df <- old_df[, -which(names(old_df) %in% c("X"))]
since <- max(old_df$status_id)
df <- search_tweets(ticker,
n,
type = 'recent',
lang = 'en',
since_id = since,
include_rts = FALSE,
retryonratelimit = TRUE)
# Merge with old file
finaldf <- rbind(old_df,df)
# Print out current ticker
print(paste(i,ticker))
# Write finaldf into newest csv file
write.csv(finaldf,paste0(path,i,'_',ticker,'_FULL.csv'))
# Save update weekly
write.csv(df,paste0(path_weekly,i,'_',ticker,'_',Sys.Date(),'.csv'))
}
# assign Twitter Auth key before crawling
TW_key <- 1
for (i in 1:nrow(coins_list)){
get_tweets(coins_list$X[i],as.character(coins_list$ticker[i]),100000, TW_key)
TW_key <- TW_key +1
if (TW_key == 5){TW_key <- 1}
}
for (i in 36:nrow(coins_list)){
get_tweets(coins_list$X[i],as.character(coins_list$ticker[i]),100000, TW_key)
TW_key <- TW_key +1
if (TW_key == 5){TW_key <- 1}
}
warnings()
