text <- str_replace_all(text, " ", "%20");
if (str_length(text) > 360){
text <- substr(text, 0, 359);
}
##########################################
data <- getURL(paste("http://api.datumbox.com/1.0/TwitterSentimentAnalysis.json?api_key=",key, "&text=",text, sep=""))
js <- fromJSON(data, asText=TRUE);
# get mood probability
sentiment = js$output$result
###################################
return(list(sentiment=sentiment))
}
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
print("Getting tweets...")
# get some tweets
tweets = searchTwitter('#iphone', 1499,since='2014-05-16', until='2014-07-01',
geocode='28.635308,77.224960,200mi',
cainfo="cacert.pem")
tweets = searchTwitter('iphone', 1499,since='2014-05-16', until='2014-07-01',
geocode='28.635308,77.224960,200mi',
cainfo="cacert.pem")
tweets = searchTwitter('#iphone', 1499,cainfo="cacert.pem")
tweets = searchTwitter('#iphone',n=1499,cainfo="cacert.pem")
tweets = searchTwitter('#iphone',lang='en'
n=1499,cainfo="cacert.pem")
tweets = searchTwitter('#iphone',lang='en',
n=1499,cainfo="cacert.pem")
# get text
tweet_txt = sapply(tweets, function(x) x$getText())
# clean text
tweet_clean = clean.text(tweet_txt)
tweet_num = length(tweet_clean)
# data frame (text, sentiment)
tweet_df = data.frame(text=tweet_clean, sentiment=rep("", tweet_num),stringsAsFactors=FALSE)
print("Getting sentiments...")
# apply function getSentiment
sentiment = rep(0, tweet_num)
for (i in 1:tweet_num)
{
tmp = getSentiment(tweet_clean[i], db_key)
tweet_df$sentiment[i] = tmp$sentiment
print(paste(i," of ", tweet_num))
}
source('D:/Research Stuff/R/WordCloudSentiment.R')
# delete rows with no sentiment
tweet_df <- tweet_df[tweet_df$sentiment!="",]
#separate text by sentiment
sents = levels(factor(tweet_df$sentiment))
#emos_label <- emos
# get the labels and percents
labels <-  lapply(sents, function(x) paste(x,format(round((length((tweet_df[tweet_df$sentiment ==x,])$text)/length(tweet_df$sentiment)*100),2),nsmall=2),"%"))
nemo = length(sents)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = tweet_df[tweet_df$sentiment == sents[i],]$text
emo.docs[i] = paste(tmp,collapse=" ")
}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords("german"))
emo.docs = removeWords(emo.docs, stopwords("english"))
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = labels
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
install.packages("devtools")
dev_mode(on=T)
install_github("ThinkToStartR",username="JulianHill",subdir="ThinkToStartR")
dev_mod(on=T)
devmod
devmode
library(devtools)
dev_mode(on=T)
install_github("ThinkToStartR",username="JulianHill",subdir="ThinkToStartR")
library(ThinkToStart)
library(ThinkToStart)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(tm)
library(wordcloud)
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
library(twitteR)
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
key<-'fd810c94bdc19bb95937acc4619351a4'
db_key<-key
library("ThinkToStartR")
library("ThinkToStart")
library(Thinktostart)
library(ThinkToStart)
require(devtools) #install if necessary (install.packages("devtools")
dev_mode(on=T)
install_github("ThinkToStartR",username="JulianHill",subdir="ThinkToStartR")
library(ThinkToStart)
library(ThinkToStartR)
ThinkToStart("SentimentCloud","nba",30,db_key)
library(twitteR)
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
ThinkToStart("SentimentCloud","nba",30,db_key)
?ThinkToStart
ThinkToStart("SentimentCloud","nba",30,db_key,
cainfo="cacert.pem")
library(twitteR)
library(tm)
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
key<-'fd810c94bdc19bb95937acc4619351a4'
db_key<-key
getSentiment <- function (text, key){
text <- URLencode(text);
#save all the spaces, then get rid of the weird characters that break the API, then convert back the URL-encoded spaces.
text <- str_replace_all(text, "%20", " ");
text <- str_replace_all(text, "%\\d\\d", "");
text <- str_replace_all(text, " ", "%20");
if (str_length(text) > 360){
text <- substr(text, 0, 359);
}
##########################################
data <- getURL(paste("http://api.datumbox.com/1.0/TwitterSentimentAnalysis.json?api_key=",db_key, "&text=",text, sep=""))
js <- fromJSON(data, asText=TRUE);
# get mood probability
sentiment = js$output$result
###################################
return(list(sentiment=sentiment))
}
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
print("Getting tweets...")
# get some tweets
tweets = searchTwitter('#iphone',lang='en',
n=1499,cainfo="cacert.pem")
# get text
tweet_txt = sapply(tweets, function(x) x$getText())
# clean text
tweet_clean = clean.text(tweet_txt)
tweet_num = length(tweet_clean)
# data frame (text, sentiment)
tweet_df = data.frame(text=tweet_clean, sentiment=rep("", tweet_num),stringsAsFactors=FALSE)
print("Getting sentiments...")
# apply function getSentiment
sentiment = rep(0, tweet_num)
for (i in 1:tweet_num)
{
tmp = getSentiment(tweet_clean[i], db_key)
tweet_df$sentiment[i] = tmp$sentiment
print(paste(i," of ", tweet_num))
}
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(tm)
library(wordcloud)
print("Getting sentiments...")
# apply function getSentiment
sentiment = rep(0, tweet_num)
for (i in 1:tweet_num)
{
tmp = getSentiment(tweet_clean[i], db_key)
tweet_df$sentiment[i] = tmp$sentiment
print(paste(i," of ", tweet_num))
}
# delete rows with no sentiment
tweet_df <- tweet_df[tweet_df$sentiment!="",]
#separate text by sentiment
sents = levels(factor(tweet_df$sentiment))
#emos_label <- emos
# get the labels and percents
labels <-  lapply(sents, function(x) paste(x,format(round((length((tweet_df[tweet_df$sentiment ==x,])$text)/length(tweet_df$sentiment)*100),2),nsmall=2),"%"))
nemo = length(sents)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = tweet_df[tweet_df$sentiment == sents[i],]$text
emo.docs[i] = paste(tmp,collapse=" ")
}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords("english"))
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = labels
png("wordcloud.png", width=1280,height=800)
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
dev.off()
wordcloud(tdm, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
library(wordcloud)
wordcloud(tdm, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
pal2 <- brewer.pal(8,"Dark2")
wordcloud(tdm, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
wordcloud(corpus, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
# get some tweets
tweets = searchTwitter('Cathay Pacific Catering Services',lang='en',
n=1000,cainfo="cacert.pem")
names(some_txt) = NULL
tweets = searchTwitter('Cathay Pacific',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific catering services',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific catering',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathay pacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('@cathaypacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=1000,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=100000,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=0,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=retryOnRateLimit,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=1,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#chowtaifook',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#chowtaifook',
n=1000,cainfo="cacert.pem")
install.packages("RFacebook")
install.packages("Rfacebook")
install.packages("devtools")
library(devtools)
install_github("Rfacebook", "pablobarbera", subdir="Rfacebook")
require (Rfacebook)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
getCheckins(LienLe, n = 10)
getCheckins(LienLe, n = 10 , token = fb_oath)
getCheckins(LienLe, n = 10 , token = fb_oauth)
me <- getUsers("me",token = fb_oauth)
my_likes <-getLikes(user="me", token = fb_oauth)
me$name
me$hometown
me$locale
me$likes
me$id
getUsers("barackobama",fb_oauth)
getUsers("barackobama",token = fb_oauth)
my_friends <- getFriends(token, simplify = TRUE)
token <- fb_oauth
my_friends <- getFriends(token, simplify = TRUE)
getFriends(token,simplify=True)
my_friends <- getFriends(token, simplify = TRUE)
head(my_friends$id, n = 1) # get lowest user ID
getGroup(AbbVieGlobal, token, n= 100)
searchGroup(AbbVieGlobal,token)
searchGroup("AbbVieGlobal",token)
searchGroup("AbbVie",token)
/Using Rfacebook package
//Using Rfacebook package
me
Using Rfacebook package
source('F:/Researches and works/R/AbbVie/Companies.R')
source('F:/Researches and works/R/AbbVie/Companies.R')
token <- fb_oauth
me <- getUsers("me", token = fb_oauth)
me
searchGroup("AbbVie",token)
getGroup(278782302258949, token)
searchGroup("AbbVie",token)
searchFacebook("AbbVie",token)
getNewsfeed(token, n= 10)
callAPI("https://graph.facebook.com/v2.0/barackobama?fields=id", token)
getGroup(278782302258949, token)
AbbVie_main <- getGroup(278782302258949, token, n = 100)
AbbVie_main
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
# visualize evolution in metric
library(ggplot2)
library(scales)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
library(ggplot2)
library(scales)
install.packages("ggplot2")
install.packages("scales")
library(ggplot2)
library(scales)
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
AbbVie_page <- getPage("AbbVie",token, n = 5000)
AbbVie_page <- getPage("AbbVieGlobal",token, n = 5000)
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
page <- getPage("humansofnewyork", token, n = 5000)
page[which.max(page$likes_count), ]
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
page <- getPage("AbbVieGlobal", token, n = 5000)
page[which.max(page$likes_count), ]
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100)) + theme_bw() + theme(axis.title.x = element_blank())
AbbVie_group <- getGroup(278782302258949, token, n = 5000)
AbbvieGlobal <- getPage("AbbVieGlobal", token, n = 5000)
searchFacebook(string ="yahoo",token, n=100)
?searchFacebook
getNewsfeed(token,n=10)
getNewsfeed(token,n=10)
searchFacebook("AbbVie",token)
saigonfirewok <- getPage("SaigonFirewok",n=100)
saigonfirewok <- getPage("SaigonFirewok",token, n=100)
searchPages("SaigonFirewok",token)
getInsights(1710766875823835,token)
getInsights(1710766875823835,token,page_impression)
getInsights(1710766875823835,token,page_impressions)
getInsights(1710766875823835,token, metric ='page_impressions')
getInsights(1506617922967149,token, metric ='page_impressions')
require (Rfacebook)
library(devtools)
# visualize evolution in metric
library(ggplot2)
library(scales)
#Table manipulation
library(dplyr)
#Get FB_Oauth
fb_oauth <- fbOAuth(app_id="204227866723896",
app_secret="e39f8a7750fd165276e0d36709201f92",
extended_permissions = TRUE)
x <- fb_oauth
searchGroup("AbbVie",x)
groupFb<- searchGroup("AbbVie",x)
View(groupFb)
AbbVie_group <- getGroup(278782302258949, x, n = 5000)
groupFb<- searchGroup("AbbVieGlobal",x)
groupFb<- searchGroup("AbbVie",x)
View(groupFb)
date_diff <- as.Date(as.character("2016/04/14"), format="%Y/%m/%d")-
as.Date(as.character("2016/01/01"), format="%Y/%m/%d")
date_diff
as.Date(as.character("2016/12/31"), format="%Y/%m/%d")-
as.Date(as.character("2016/10/01"), format="%Y/%m/%d")
as.Date(as.character("2015/12/31"), format="%Y/%m/%d")-
as.Date(as.character("2015/10/15"), format="%Y/%m/%d")
# install packages if not available
packages <- c('twitteR', 'tesseract', '	abbyyR','data.table','dplyr')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR()
setwd("C:/Users/BluePhoenix/Documents/GitHub/NextBigCrypto-Senti/Crawlers")
coins_list <- read.csv("Top50_Oct7.csv")
View(coins_list)
