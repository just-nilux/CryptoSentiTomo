x %in% c(4,5,6) -> '04',
x %in% c(7,8,9) -> '07',
x %in% c(10,11,12) -> '10'
)
}
# sampling
Q <- lubridate::quarter(Sys.Date())
Y <- lubridate::year(Sys.Date())
W <- lubridate::isoweek(Sys.Date()) - lubridate::isoweek(paste0(year(Sys.Date()),'-',detect_week(month(Sys.Date())),'-01'))
if (weekdays(as.Date(Sys.Date())) %in% c('Friday','Saturday','Sunday')){W <- W + 1}
#
ifelse (W == 1,LW <- isoweek(Sys.Date()-7) - isoweek(paste0(year(Sys.Date()-7),'-',detect_week(month(Sys.Date()-7)),'-01')), LW <- W - 1)
if (Q==1){
LQ <- 4
Y <- Y - 1
} else {LQ <- Q -1}
#Last year
LY <- Y - 1
#############################################################################################################
#############################################################################################################
###                                       Directions of coefficients                                      ###
#############################################################################################################
#############################################################################################################
Vorzeichen <- as.vector(c(1,1,1,1,1,1,1, 1,1,1,1,1,1,1, 1,1,1,1,1,1,1, # 3*(6+1) OPP open values
1,1,1,1,1,1,1, 1,1,1,1,1,1,1, 1,1,1,1,1,1,1, # 3*(6+1) OPP won/booekd values
-1,-1,-1,-1,-1,-1,-1,  -1,-1,-1,-1,-1,-1,-1,  -1,-1,-1,-1,-1,-1,-1, # 3*(6+1) OPP discontinued values
1,1,1,1,0,0,1, 1,1, 1, # 7*ADRM/Budget derivates 2*historic actuals 1*growth
1,1,1) # 3*wb rates
)
# --current Quarter Total
Data_sample <- Data_sort2[(Data_sort2$YEAR < Y | (Data_sort2$YEAR == Y & Data_sort2$QUARTER < Q)), ]
Data_sample <- Data_sample[(Data_sample$YEAR > 2014 | (Data_sample$YEAR == 2014 & Data_sample$QUARTER > 4)), ]
Data_sample <- Data_sample[(Data_sample$ENTITY != 'EMEA-North'), ]
Data_sample <- Data_sample[(Data_sample$ENTITY != 'EMEA-South'), ]
Data_sample_nm_cur <- Data_sample
Data_sample_nm_cur[is.na(Data_sample_nm_cur)] <- 0
Data_sample_fc_cur <- Data_sort2[Data_sort2$YEAR == Y, ]
Data_sample_fc_cur <- Data_sample_fc_cur[Data_sample_fc_cur$QUARTER == Q, ]
Data_sample_fc_cur[is.na(Data_sample_fc_cur)] <- 0
DV2 <- as.numeric(unlist(Data_sample_nm_cur["DV"]))
M <-as.matrix(Data_sample_nm_cur[,7:ncol(Data_sample_nm_cur)])
M[is.infinite(M)] <- 0
coef_cur_total <-coef(nnnpls(M, DV2, Vorzeichen))
names(coef_cur_total) <- colnames(Data_sample_fc_cur[,7:ncol(Data_sample_fc_cur)])
###################################################################
# --CURRENT Quarter Region
Data_sample <- Data_sort2_reg[(Data_sort2_reg$YEAR < Y | (Data_sort2_reg$YEAR == Y & Data_sort2_reg$QUARTER < Q)), ]
Data_sample <- Data_sample[(Data_sample$YEAR > 2014 | (Data_sample$YEAR == 2014 & Data_sample$QUARTER > 4)), ]
Data_sample_nm_cur <- Data_sample
Data_sample_nm_cur[is.na(Data_sample_nm_cur)] <- 0
Data_sample_fc_cur <- Data_sort2_reg[Data_sort2_reg$YEAR == Y, ]
Data_sample_fc_cur <- Data_sample_fc_cur[Data_sample_fc_cur$QUARTER == Q, ]
Data_sample_fc_cur[is.na(Data_sample_fc_cur)] <- 0
DV2 <- as.numeric(unlist(Data_sample_nm_cur["DV"]))
M <-as.matrix(Data_sample_nm_cur[,7:ncol(Data_sample_nm_cur)])
M[is.infinite(M)] <- 0
coef_cur_reg <-coef(nnnpls(M, DV2, Vorzeichen))
names(coef_cur_reg) <- colnames(Data_sample_fc_cur[,7:ncol(Data_sample_fc_cur)])
####################################################################
# --LAST Quarter Total
Data_sample <- Data_sort2[(Data_sort2$YEAR < Y | (Data_sort2$YEAR == Y & Data_sort2$QUARTER < LQ)), ]
Data_sample <- Data_sample[(Data_sample$YEAR > 2014 | (Data_sample$YEAR == 2014 & Data_sample$QUARTER > 4)), ]
Data_sample <- Data_sample[(Data_sample$ENTITY != 'EMEA-North'), ]
Data_sample <- Data_sample[(Data_sample$ENTITY != 'EMEA-South'), ]
Data_sample_nm_cur <- Data_sample
Data_sample_nm_cur[is.na(Data_sample_nm_cur)] <- 0
Data_sample_fc_cur <- Data_sort2[Data_sort2$YEAR == Y, ]
Data_sample_fc_cur <- Data_sample_fc_cur[Data_sample_fc_cur$QUARTER == LQ, ]
Data_sample_fc_cur[is.na(Data_sample_fc_cur)] <- 0
DV2 <- as.numeric(unlist(Data_sample_nm_cur["DV"]))
M <-as.matrix(Data_sample_nm_cur[,7:ncol(Data_sample_nm_cur)])
M[is.infinite(M)] <- 0
coef_last_total <-coef(nnnpls(M, DV2, Vorzeichen))
names(coef_last_total) <- colnames(Data_sample_fc_cur[,7:ncol(Data_sample_fc_cur)])
###################################################################
# --LAST Quarter Region
Data_sample <- Data_sort2_reg[(Data_sort2_reg$YEAR < Y | (Data_sort2_reg$YEAR == Y & Data_sort2_reg$QUARTER < LQ)), ]
Data_sample <- Data_sample[(Data_sample$YEAR > 2014 | (Data_sample$YEAR == 2014 & Data_sample$QUARTER > 4)), ]
Data_sample_nm_cur <- Data_sample
Data_sample_nm_cur[is.na(Data_sample_nm_cur)] <- 0
Data_sample_fc_cur <- Data_sort2_reg[Data_sort2_reg$YEAR == Y, ]
Data_sample_fc_cur <- Data_sample_fc_cur[Data_sample_fc_cur$QUARTER == LQ, ]
Data_sample_fc_cur[is.na(Data_sample_fc_cur)] <- 0
DV2 <- as.numeric(unlist(Data_sample_nm_cur["DV"]))
M <-as.matrix(Data_sample_nm_cur[,7:ncol(Data_sample_nm_cur)])
M[is.infinite(M)] <- 0
coef_last_reg <-coef(nnnpls(M, DV2, Vorzeichen))
names(coef_last_reg) <- colnames(Data_sample_fc_cur[,7:ncol(Data_sample_fc_cur)])
####################################################################
# --LAST YEAR Total
Data_sample <- Data_sort2[(Data_sort2$YEAR < LY | (Data_sort2$YEAR == LY & Data_sort2$QUARTER < Q)), ]
Data_sample <- Data_sample[(Data_sample$YEAR > 2014 | (Data_sample$YEAR == 2014 & Data_sample$QUARTER > 4)), ]
Data_sample <- Data_sample[(Data_sample$ENTITY != 'EMEA-North'), ]
Data_sample <- Data_sample[(Data_sample$ENTITY != 'EMEA-South'), ]
Data_sample_nm_cur <- Data_sample
Data_sample_nm_cur[is.na(Data_sample_nm_cur)] <- 0
Data_sample_fc_cur <- Data_sort2[Data_sort2$YEAR == LY, ]
Data_sample_fc_cur <- Data_sample_fc_cur[Data_sample_fc_cur$QUARTER == Q, ]
Data_sample_fc_cur[is.na(Data_sample_fc_cur)] <- 0
DV2 <- as.numeric(unlist(Data_sample_nm_cur["DV"]))
M <-as.matrix(Data_sample_nm_cur[,7:ncol(Data_sample_nm_cur)])
M[is.infinite(M)] <- 0
coef_last_y_total <-coef(nnnpls(M, DV2, Vorzeichen))
names(coef_last_y_total) <- colnames(Data_sample_fc_cur[,7:ncol(Data_sample_fc_cur)])
###################################################################
# --LAST YEAR Region
Data_sample <- Data_sort2_reg[(Data_sort2_reg$YEAR < LY | (Data_sort2_reg$YEAR == LY & Data_sort2_reg$QUARTER < Q)), ]
Data_sample <- Data_sample[(Data_sample$YEAR > 2014 | (Data_sample$YEAR == 2014 & Data_sample$QUARTER > 4)), ]
Data_sample_nm_cur <- Data_sample
Data_sample_nm_cur[is.na(Data_sample_nm_cur)] <- 0
Data_sample_fc_cur <- Data_sort2_reg[Data_sort2_reg$YEAR == LY, ]
Data_sample_fc_cur <- Data_sample_fc_cur[Data_sample_fc_cur$QUARTER == Q, ]
Data_sample_fc_cur[is.na(Data_sample_fc_cur)] <- 0
DV2 <- as.numeric(unlist(Data_sample_nm_cur["DV"]))
M <-as.matrix(Data_sample_nm_cur[,7:ncol(Data_sample_nm_cur)])
M[is.infinite(M)] <- 0
coef_last_y_reg <-coef(nnnpls(M, DV2, Vorzeichen))
names(coef_last_y_reg) <- colnames(Data_sample_fc_cur[,7:ncol(Data_sample_fc_cur)])
#######################################################################################
#specify sets for different regions/markets/sales bags
check_set <- summarize(group_by(Data_sort2,REGION,ENTITY))
#report building for each region/MU/SB
report_building <- function (a,b)
{
current <- dplyr::filter(Data_sort2,
Data_sort2$REGION == a &
Data_sort2$ENTITY == b &
Data_sort2$YEAR == Y &
Data_sort2$QUARTER == Q &
Data_sort2$WEEK == W)
currentx <- melt(current,id.vars =  c("REGION","ENTITY"), value.name = "VALUE")
last_quarter <- dplyr::filter(Data_sort2,
Data_sort2$REGION == a &
Data_sort2$ENTITY == b &
Data_sort2$YEAR == Y &
Data_sort2$QUARTER == LQ &
Data_sort2$WEEK == W)
last_quarterx <- melt(last_quarter,id.vars =  c("REGION","ENTITY"), value.name = "VALUE")
last_year <- dplyr::filter(Data_sort2,
Data_sort2$REGION == a &
Data_sort2$ENTITY == b &
Data_sort2$YEAR == LY &
Data_sort2$QUARTER == Q &
Data_sort2$WEEK == W)
last_yearx <- melt(last_year,id.vars =  c("REGION","ENTITY"), value.name = "VALUE")
# check whether LW belong to last quarter
LWQ <- 0
ifelse (W==1, LWQ <- Q - 1, LWQ <- Q)
last_week <- dplyr::filter(Data_sort2,
Data_sort2$REGION == a &
Data_sort2$ENTITY == b &
Data_sort2$YEAR == Y &
Data_sort2$QUARTER == LWQ &
Data_sort2$WEEK == LW)
last_weekx <- melt(last_week,id.vars =  c("REGION","ENTITY"), value.name = "VALUE")
finalx <- data.frame()
finalx <- cbind(currentx,last_quarterx[,4],last_weekx[,4],last_yearx[,4])
colnames(finalx) <- c("REGION","ENTITY","VARIABLES","CURRENT_WEEK","LAST_QUARTER","LAST_WEEK","LAST_YEAR")
return(finalx)
}
# apply the list for region/MU/SB to obtain the reports respectively
extraction <- function(df)
{
final <- data.frame(REGION = character, ENTITY = character,
VARIABLES = character, CURRENT_WEEK = double, LAST_QUARTER = double,
LAST_WEEK = double, LAST_YEAR = double)
for (i in 1:nrow(df))
{
a <- as.character(df[i,1])
b <- as.character(df[i,2])
c <- try(report_building(a,b))
if (class(c)=="try-error") next;
final <- try(rbind(final,c))
if (class(final)=="try-error") next;
}
return(final)
}
finaldf <- data.frame(REGION = character, ENTITY = character,
VARIABLES = character, CURRENT_WEEK = double, LAST_QUARTER = double, LAST_WEEK = double, LAST_YEAR = double)
finaldf <- rbind(finaldf,extraction(check_set))
finaldf <- data.frame(REGION = character, ENTITY = character,
VARIABLES = character, CURRENT_WEEK = double, LAST_QUARTER = double, LAST_WEEK = double, LAST_YEAR = double)
# Detect week of quarter
detect_week <- function(x)
{
memisc::cases(
x %in% c(1,2,3) -> '01',
x %in% c(4,5,6) -> '04',
x %in% c(7,8,9) -> '07',
x %in% c(10,11,12) -> '10'
)
}
# sampling
Q <- lubridate::quarter(Sys.Date())
Y <- lubridate::year(Sys.Date())
W <- lubridate::isoweek(Sys.Date()) - lubridate::isoweek(paste0(year(Sys.Date()),'-',detect_week(month(Sys.Date())),'-01'))
if (weekdays(as.Date(Sys.Date())) %in% c('Friday','Saturday','Sunday')){W <- W + 1}
#
ifelse (W == 1,LW <- isoweek(Sys.Date()-7) - isoweek(paste0(year(Sys.Date()-7),'-',detect_week(month(Sys.Date()-7)),'-01')), LW <- W - 1)
if (Q==1){
LQ <- 4
Y <- Y - 1
} else {LQ <- Q -1}
#Last year
LY <- Y - 1
install.packages("bittrex")
library("bittrex")
library(bittrex)
library(jsonlite)
library(httr)
library(digest)
api.poloniex <- function(key, secret, command, args = list()) {
req <- c(list(
command = command,
nonce = round(as.numeric(Sys.time()) * 1e4, 0)),
args)
ret <- POST("https://poloniex.com/tradingApi",
add_headers(key=key, sign=hmac(secret, httr:::compose_query(req), "sha512")),
body = req,
encode = "form")
stop_for_status(ret)
content(ret)
}
api.poloniex(exchanges$poloniex$key, exchanges$poloniex$secret, "returnTradeHistory", list(currencyPair = "all"))
devtools::install_github("ropensci/bittrex")
?DT
??DT
packages <- c('rtweet','twitteR', #Twitter API crawlers
'tesseract', 'magick', #image processing
'data.table','dplyr','scales','ggplot2','taskscheduleR',
'httr','stringr','rvest','curl','lubridate')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
packages <- c('rtweet','twitteR', #Twitter API crawlers
'tesseract', 'magick', #image processing
'data.table','dplyr','scales','ggplot2','taskscheduleR',
'httr','stringr','rvest','curl','lubridate')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
consumer_key <- 'cdi1LlwgzdXzR4Wxz8T3Gude6'
consumer_secret <- 's3hpLYXs9ULY1YwzyTRP8aRovp3rvkjUM9ue9usi8MotrvUgOG'
access_token <- '240771509-MQiqGMegj3B4ohmRSi7mThfprMg7j9lAYkDB3s9W'
access_token_secret <- 'YZGMyJ6Jz3Ncx1SG59QXJGaRrEkYjvCTC71KPsFH2eaIi'
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
test <- search_tweets("ETH", n = 10)
test <- search_tweets("ETH", n = 10)
library(ploty)
install.packages("ploty")
install.packages("mlbench")
library(plotly)
install.packages("plotly")
# Try APIs call on https://elasticsearch.pushshift.io
#Set up working directory
setwd("~/GitHub/NextBigCrypto-Senti/Crawlers")
# Clear environment
rm(list = ls())
# install packages if not available
packages <- c("jsonlite","httr", # support API call
"dplyr")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
#--------------------------------#
# GENESIS CODE (run only 1 time) #
#--------------------------------#
#
# url <- "https://api.pushshift.io"
# path <- "/reddit/search/submission/"
# query <- "?q=&subreddit=cryptomarkets&before=1509647464&stickied=false&locked=false&is_video=false&size=500"
#
# # Extract Reddit submissions using filters
# # - Subreddit = cryptomarkets
# # - From 1st Jan - Now (epoch time)
# # - No sticked, locked and video submissions
# # - Size = 500 (maximum per call)
#
# raw_data <- GET(url = url,
#                 path = path,
#                 query = query)
# names(raw_data)
# raw_data$status_code #if status = 200 -> fine
#
# test <- rawToChar(raw_data$content)
#
# # extract JSON type from raw_data
# extract.data <- fromJSON(test)
# class(extract.data)
#
# # convert to dataframe
# reddit.df <- do.call(what = "rbind",
#                      args = lapply(extract.data, as.data.frame))
# # drop nested columns
# reddit.df <- reddit.df[ , -which(names(reddit.df) %in%
#                                    c("media","media_embed","preview",
#                                      "secure_media","secure_media_embed"))]
# report_path <- '2b. Reddit Report/'
# # save genesis dataset
# write.csv(reddit.df,paste0(report_path,'CryptoMarkets_Reddit.csv'))
#---------------------------------------------------------------------
# Convert epoch time
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
# Develop function to re-loop crawl from pushshift.io
# base on last epoch date available in the dataset up to current date
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&after=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
# Get current sys.time in epoch format
end_epoch <- as.integer(as.POSIXct(Sys.time()))
# Continue to crawl Reddit until epoch time reach 1st Nov
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"CryptoMarkets_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.numeric(max(old.df$created_utc, na.rm = TRUE))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
# Check if new.df is null --> break
if (dim(new.df)[2] == 0) {
print('Finish Crawling')
break
}
# merge old + new --> final df
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'CryptoMarkets_Reddit_',
substr(unix2POSIXct(max_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(max_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'CryptoMarkets_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
# -------------------------------------------------------------
# Can only crawl back until 3rd May 2017 due to API limitation
typeof(old.df)
typeof(new.df)
new.df.test <- as.data.frame(new.df)
View(new.df.test)
# Keep only list of columns same as old.df
old_col <- colnames(old.df)
# Continue to crawl Reddit until epoch time reach 1st Nov
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"CryptoMarkets_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.numeric(max(old.df$created_utc, na.rm = TRUE))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
# Keep only list of columns same as old.df
old_col <- colnames(old.df)
new.df <- new.df[ , which(names(new.df) %in% old_col)]
# Check if new.df is null --> break
if (dim(new.df)[2] == 0) {
print('Finish Crawling')
break
}
# merge old + new --> final df
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'CryptoMarkets_Reddit_',
substr(unix2POSIXct(max_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(max_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'CryptoMarkets_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
# Try APIs call on https://elasticsearch.pushshift.io
#Set up working directory
setwd("~/GitHub/NextBigCrypto-Senti/Crawlers")
# Clear environment
rm(list = ls())
# install packages if not available
packages <- c("jsonlite","httr", # support API call
"dplyr")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
#--------------------------------#
# GENESIS CODE (run only 1 time) #
#--------------------------------#
#
# url <- "https://api.pushshift.io"
# path <- "/reddit/search/submission/"
# query <- "?q=&subreddit=cryptocurrency&after=1506816000&stickied=false&locked=false&is_video=false&size=500"
#
# # Extract Reddit submissions using filters
# # - Subreddit = cryptocurrency
# # - After = 1st Oct 2017 (epoch time)
# # - No sticked, locked and video submissions
# # - Size = 500 (maximum per call)
#
# raw_data <- GET(url = url,
#                 path = path,
#                 query = query)
# names(raw_data)
# raw_data$status_code #if status = 200 -> fine
#
# test <- rawToChar(raw_data$content)
#
# # extract JSON type from raw_data
# extract.data <- fromJSON(test)
# class(extract.data)
#
# # convert to dataframe
# reddit.df <- do.call(what = "rbind",
#                      args = lapply(extract.data, as.data.frame))
# # drop nested columns
# reddit.df <- reddit.df[ , -which(names(reddit.df) %in%
#                                    c("media","media_embed","preview",
#                                      "secure_media","secure_media_embed"))]
# report_path <- '2b. Reddit Report/'
# # save genesis dataset
# write.csv(reddit.df,paste0(report_path,'Crypto_Reddit.csv'))
#---------------------------------------------------------------------
# Convert epoch time
unix2POSIXct <- function(time) structure(time, class = c("POSIXt", "POSIXct"))
# Develop function to re-loop crawl from pushshift.io
# base on last epoch date available in the dataset up to current date
crawl_reddit_pushshift <- function(epoch){
url <- "https://api.pushshift.io"
path <- "/reddit/search/submission/"
query <- "?q=&subreddit=cryptocurrency&stickied=false&locked=false&is_video=false&size=500"
# query results after epoch time
query <- paste0(query,'&after=',epoch)
# extract data from API
raw_data <- GET(url = url,
path = path,
query = query)
# will implement confirm mechanism later if status code <> 200
raw_data$status_code #if status = 200 -> fine
char_data <- rawToChar(raw_data$content)
# extract JSON type from raw_data
extract.data <- fromJSON(char_data)
# convert to dataframe
final.df <- do.call(what = "rbind",
args = lapply(extract.data, as.data.frame))
# drop nested columns
final.df <- final.df[ , -which(names(final.df) %in%
c("media","media_embed","preview",
"secure_media","secure_media_embed"))]
return(final.df)
}
# Get current sys.time in epoch format
end_epoch <- as.integer(as.POSIXct(Sys.time()))
# Continue to crawl Reddit until epoch time reach 1st Nov
repeat{
# Extract data from previous report
report_path <- '2b. Reddit Report/'
old.df <- read.csv(paste0(report_path,"Crypto_Reddit.csv"))
old.df <- old.df[ , -which(names(old.df) %in% c("X"))]
# extract latest epoch time from old dataset
max_epoch <- as.numeric(max(old.df$created_utc,na.rm = TRUE))
# Start crawling and save to "Crypto_Reddit.csv"
new.df <- crawl_reddit_pushshift(max_epoch)
# Keep only list of columns same as old.df
old_col <- colnames(old.df)
new.df <- new.df[ , which(names(new.df) %in% old_col)]
# Check if new.df is null --> break
if (dim(new.df)[2] == 0) {
print('Finish Crawling')
break
}
# merge old + new --> final df
final.df <- bind_rows(old.df,new.df)
# Save new.df base on epoch
write.csv(new.df,paste0(report_path,'Crypto_Reddit_',
substr(unix2POSIXct(max_epoch),0,10)
,'.csv'))
print(substr(unix2POSIXct(max_epoch),0,10))
# Overwrite final file
write.csv(final.df,paste0(report_path,'Crypto_Reddit.csv'))
if(max_epoch >= end_epoch){
break
}
}
# -------------------------------------------------------------
# Can only crawl back until 3rd May 2017 due to API limitation
