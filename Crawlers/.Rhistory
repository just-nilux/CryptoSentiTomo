if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
print("Getting tweets...")
# get some tweets
tweets = searchTwitter('#iphone',lang='en',
n=1499,cainfo="cacert.pem")
# get text
tweet_txt = sapply(tweets, function(x) x$getText())
# clean text
tweet_clean = clean.text(tweet_txt)
tweet_num = length(tweet_clean)
# data frame (text, sentiment)
tweet_df = data.frame(text=tweet_clean, sentiment=rep("", tweet_num),stringsAsFactors=FALSE)
print("Getting sentiments...")
# apply function getSentiment
sentiment = rep(0, tweet_num)
for (i in 1:tweet_num)
{
tmp = getSentiment(tweet_clean[i], db_key)
tweet_df$sentiment[i] = tmp$sentiment
print(paste(i," of ", tweet_num))
}
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(tm)
library(wordcloud)
print("Getting sentiments...")
# apply function getSentiment
sentiment = rep(0, tweet_num)
for (i in 1:tweet_num)
{
tmp = getSentiment(tweet_clean[i], db_key)
tweet_df$sentiment[i] = tmp$sentiment
print(paste(i," of ", tweet_num))
}
# delete rows with no sentiment
tweet_df <- tweet_df[tweet_df$sentiment!="",]
#separate text by sentiment
sents = levels(factor(tweet_df$sentiment))
#emos_label <- emos
# get the labels and percents
labels <-  lapply(sents, function(x) paste(x,format(round((length((tweet_df[tweet_df$sentiment ==x,])$text)/length(tweet_df$sentiment)*100),2),nsmall=2),"%"))
nemo = length(sents)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = tweet_df[tweet_df$sentiment == sents[i],]$text
emo.docs[i] = paste(tmp,collapse=" ")
}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords("english"))
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = labels
png("wordcloud.png", width=1280,height=800)
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
dev.off()
wordcloud(tdm, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
library(wordcloud)
wordcloud(tdm, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
pal2 <- brewer.pal(8,"Dark2")
wordcloud(tdm, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
wordcloud(corpus, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
# get some tweets
tweets = searchTwitter('Cathay Pacific Catering Services',lang='en',
n=1000,cainfo="cacert.pem")
names(some_txt) = NULL
tweets = searchTwitter('Cathay Pacific',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific catering services',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific catering',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('cathay pacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathay pacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('@cathaypacific',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=1000,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=100000,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=0,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=retryOnRateLimit,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#cathaypacific',lang='en',retryOnRateLimit=1,
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#chowtaifook',lang='en',
n=1000,cainfo="cacert.pem")
tweets = searchTwitter('#chowtaifook',
n=1000,cainfo="cacert.pem")
install.packages("RFacebook")
install.packages("Rfacebook")
install.packages("devtools")
library(devtools)
install_github("Rfacebook", "pablobarbera", subdir="Rfacebook")
require (Rfacebook)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="204227866723896", app_secret="e39f8a7750fd165276e0d36709201f92",extended_permissions = TRUE)
getCheckins(LienLe, n = 10)
getCheckins(LienLe, n = 10 , token = fb_oath)
getCheckins(LienLe, n = 10 , token = fb_oauth)
me <- getUsers("me",token = fb_oauth)
my_likes <-getLikes(user="me", token = fb_oauth)
me$name
me$hometown
me$locale
me$likes
me$id
getUsers("barackobama",fb_oauth)
getUsers("barackobama",token = fb_oauth)
my_friends <- getFriends(token, simplify = TRUE)
token <- fb_oauth
my_friends <- getFriends(token, simplify = TRUE)
getFriends(token,simplify=True)
my_friends <- getFriends(token, simplify = TRUE)
head(my_friends$id, n = 1) # get lowest user ID
getGroup(AbbVieGlobal, token, n= 100)
searchGroup(AbbVieGlobal,token)
searchGroup("AbbVieGlobal",token)
searchGroup("AbbVie",token)
/Using Rfacebook package
//Using Rfacebook package
me
Using Rfacebook package
source('F:/Researches and works/R/AbbVie/Companies.R')
source('F:/Researches and works/R/AbbVie/Companies.R')
token <- fb_oauth
me <- getUsers("me", token = fb_oauth)
me
searchGroup("AbbVie",token)
getGroup(278782302258949, token)
searchGroup("AbbVie",token)
searchFacebook("AbbVie",token)
getNewsfeed(token, n= 10)
callAPI("https://graph.facebook.com/v2.0/barackobama?fields=id", token)
getGroup(278782302258949, token)
AbbVie_main <- getGroup(278782302258949, token, n = 100)
AbbVie_main
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
# visualize evolution in metric
library(ggplot2)
library(scales)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
library(ggplot2)
library(scales)
install.packages("ggplot2")
install.packages("scales")
library(ggplot2)
library(scales)
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
AbbVie_page <- getPage("AbbVie",token, n = 5000)
AbbVie_page <- getPage("AbbVieGlobal",token, n = 5000)
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
page <- getPage("humansofnewyork", token, n = 5000)
page[which.max(page$likes_count), ]
## convert Facebook date format to R date format
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100, 1000, 10000, 50000)) + theme_bw() + theme(axis.title.x = element_blank())
page <- getPage("AbbVieGlobal", token, n = 5000)
page[which.max(page$likes_count), ]
format.facebook.date <- function(datestring) {
date <- as.POSIXct(datestring, format = "%Y-%m-%dT%H:%M:%S+0000", tz = "GMT")
}
## aggregate metric counts over month
aggregate.metric <- function(metric) {
m <- aggregate(page[[paste0(metric, "_count")]], list(month = page$month),
mean)
m$month <- as.Date(paste0(m$month, "-15"))
m$metric <- metric
return(m)
}
# create data frame with average metric counts per month
page$datetime <- format.facebook.date(page$created_time)
page$month <- format(page$datetime, "%Y-%m")
df.list <- lapply(c("likes", "comments", "shares"), aggregate.metric)
df <- do.call(rbind, df.list)
ggplot(df, aes(x = month, y = x, group = metric)) + geom_line(aes(color = metric)) +
scale_x_date(date_breaks = "years",
labels = date_format("%Y")) + scale_y_log10("Average count per post",
breaks = c(10, 100)) + theme_bw() + theme(axis.title.x = element_blank())
AbbVie_group <- getGroup(278782302258949, token, n = 5000)
AbbvieGlobal <- getPage("AbbVieGlobal", token, n = 5000)
searchFacebook(string ="yahoo",token, n=100)
?searchFacebook
getNewsfeed(token,n=10)
getNewsfeed(token,n=10)
searchFacebook("AbbVie",token)
saigonfirewok <- getPage("SaigonFirewok",n=100)
saigonfirewok <- getPage("SaigonFirewok",token, n=100)
searchPages("SaigonFirewok",token)
getInsights(1710766875823835,token)
getInsights(1710766875823835,token,page_impression)
getInsights(1710766875823835,token,page_impressions)
getInsights(1710766875823835,token, metric ='page_impressions')
getInsights(1506617922967149,token, metric ='page_impressions')
require (Rfacebook)
library(devtools)
# visualize evolution in metric
library(ggplot2)
library(scales)
#Table manipulation
library(dplyr)
#Get FB_Oauth
fb_oauth <- fbOAuth(app_id="204227866723896",
app_secret="e39f8a7750fd165276e0d36709201f92",
extended_permissions = TRUE)
x <- fb_oauth
searchGroup("AbbVie",x)
groupFb<- searchGroup("AbbVie",x)
View(groupFb)
AbbVie_group <- getGroup(278782302258949, x, n = 5000)
groupFb<- searchGroup("AbbVieGlobal",x)
groupFb<- searchGroup("AbbVie",x)
View(groupFb)
date_diff <- as.Date(as.character("2016/04/14"), format="%Y/%m/%d")-
as.Date(as.character("2016/01/01"), format="%Y/%m/%d")
date_diff
as.Date(as.character("2016/12/31"), format="%Y/%m/%d")-
as.Date(as.character("2016/10/01"), format="%Y/%m/%d")
as.Date(as.character("2015/12/31"), format="%Y/%m/%d")-
as.Date(as.character("2015/10/15"), format="%Y/%m/%d")
# install packages if not available
packages <- c('twitteR', 'tesseract', '	abbyyR','data.table','dplyr')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR()
options("scipen"=100, "digits"=4)
# Set up working directory
setwd("~/GitHub/NextBigCrypto-Senti/Crawlers")
# Clear environment
rm(list = ls())
# devtools::install_github("mkearney/rtweet")
# install packages if not available
packages <- c('jsonlite','plyr', 'dplyr', 'doSNOW','doParallel','lubridate','coinmarketcapr')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
getCoins <- function() {
library(plyr)
today <- gsub("-", "", today())
json <- "https://files.coinmarketcap.com/generated/search/quick_search.json"
coins <- jsonlite::read_json(json, simplifyVector = TRUE)
coins <- data_frame(symbol = coins$symbol, name = coins$name, slug = coins$slug,
rank = coins$rank)
length <- as.numeric(length(coins$slug))
range <- 1:length
url <- paste0("https://coinmarketcap.com/currencies/", coins$slug, "/historical-data/?start=20130428&end=",
today)
baseurl <- c(url)
coins$slug <- as.character(baseurl)
coins$rank <- as.numeric(coins$rank)
return(coins)
}
# Scrape Historical Tables -----
abstracts <- function(attributes) {
page <- read_html(attributes)
names <- page %>% html_nodes(css = ".col-sm-4 .text-large") %>% html_text(trim = TRUE) %>%
replace(!nzchar(.), NA)
nodes <- page %>% html_nodes(css = "table") %>% .[1] %>% html_table(fill = TRUE) %>%
replace(!nzchar(.), NA)
abstracts <- Reduce(rbind, nodes)
abstracts$symbol <- gsub("\\(||\\n|\\)|\\s\\s", "", names)
abstracts$symbol <- as.character(strsplit(abstracts$symbol, " ")[[1]][1])
return(abstracts)
}
# Cleanup results table -----
cleanUp <- function(results) {
names(results) <- c("symbol", "date", "open", "high", "low", "close", "volume",
"market", "name", "ranknow")
marketdata <- results
marketdata$volume <- gsub("\\,", "", marketdata$volume)
marketdata$market <- gsub("\\,", "", marketdata$market)
marketdata$volume <- gsub("\\-", "0", marketdata$volume)
marketdata$market <- gsub("\\-", "0", marketdata$market)
marketdata$close <- gsub("\\-", "0", marketdata$close)
marketdata$date <- format(strptime(marketdata$date, format = "%b %d,%Y"), "%Y-%m-%d")
marketdata$open <- as.numeric(marketdata$open)
marketdata$close <- as.numeric(marketdata$close)
marketdata$high <- as.numeric(marketdata$high)
marketdata$low <- as.numeric(marketdata$low)
marketdata$volume <- as.numeric(marketdata$volume)
marketdata$market <- as.numeric(marketdata$market)
# Percent variance between open and close rates
marketdata$variance <- ((marketdata$close - marketdata$open)/marketdata$close)
# spread variance between days high, low and closing
marketdata$volatility <- ((marketdata$high - marketdata$low)/marketdata$close)
return(marketdata)
}
# START CRYPTOCURRENCY SCRAPING SCRIPT ------------------------------------
# Crypto Scraping Setup ---------------------------------------------------
file <- "Crypto-Markets.csv"
coins <- getCoins()
length <- as.numeric(length(coins$slug))
View(coins)
coins_list <- read.csv("Top50_Oct7.csv")
# Get top 50 coins at Oct 7 only
# coins <- getCoins()
coins <- read.csv("Top50_Oct7.csv")
length <- as.numeric(length(coins$slug))
range <- 1:length
cpucore <- as.numeric(detectCores(all.tests = FALSE, logical = TRUE))
ptm <- proc.time()
# START CRYPTOCURRENCY SCRAPING SCRIPT ------------------------------------
# Crypto Scraping Setup ---------------------------------------------------
file <- paste0("Crypto-Markets_",Sys.Date,".csv")
# START CRYPTOCURRENCY SCRAPING SCRIPT ------------------------------------
# Crypto Scraping Setup ---------------------------------------------------
file <- paste0('Crypto-Markets_',Sys.Date,'.csv')
# START CRYPTOCURRENCY SCRAPING SCRIPT ------------------------------------
# Crypto Scraping Setup ---------------------------------------------------
file <- paste0('Crypto-Markets_',Sys.Date(),'.csv')
# Get top 50 coins at Oct 7 only
# coins <- getCoins()
coins <- read.csv("Top50_Oct7.csv")
length <- as.numeric(length(coins$slug))
range <- 1:length
cpucore <- as.numeric(detectCores(all.tests = FALSE, logical = TRUE))
ptm <- proc.time()
# Uncomment for fiat exchange rate ----- exchange_rate <-
fromJSON('https://api.fixer.io/latest?base=EUR')
AUD <- exchange_rate$rates$AUD
# Uncomment for fiat exchange rate -----
exchange_rate <- fromJSON('https://api.fixer.io/latest?base=USD')
EUR <- exchange_rate$rates$EUR
EUR
length <- as.numeric(length(coins$slug))
range <- 1:length
cpucore <- as.numeric(detectCores(all.tests = FALSE, logical = TRUE))
ptm <- proc.time()
# Uncomment for fiat exchange rate -----
exchange_rate <- fromJSON('https://api.fixer.io/latest?base=USD')
EUR <- exchange_rate$rates$EUR
# Parallel process scraping with progress bar -----------------------------
cluster = makeCluster(cpucore, type = "SOCK")
registerDoSNOW(cluster)
pb <- txtProgressBar(max = length, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
attributes <- coins$slug
# Combine results and stop clusters ---------------------------------------
results = foreach(i = range, .options.snow = opts, .combine = rbind, .packages = "rvest") %dopar%
abstracts(attributes[i])
close(pb)
stopCluster(cluster)
# Cleanup results and fix names -------------------------------------------
coinnames <- data_frame(symbol = coins$symbol, name = coins$name, rank = coins$rank)
results <- merge(results, coinnames)
marketdata <- cleanUp(results)
write.csv(marketdata, file, row.names = FALSE)
print(proc.time() - ptm)
View(exchange_rate)
pb <- txtProgressBar(max = length, style = 3)
length <- as.numeric(length(coins$slug))
# Get top 50 coins at Oct 7 only
coins <- getCoins()
View(coins)
View(coins_list)
View(coins)
View(coins_list)
View(coins)
rm(coins_list)
coins <- inner_join(coins,coins50)
coins50 <- read.csv("Top50_Oct7.csv")
coins <- inner_join(coins,coins50)
View(coins)
coins <- getCoins()
coins50 <- read.csv("Top50_Oct7.csv")
coins50 <- coins50[,-1]
coins <- inner_join(coins,coins50)
length <- as.numeric(length(coins$slug))
range <- 1:length
cpucore <- as.numeric(detectCores(all.tests = FALSE, logical = TRUE))
ptm <- proc.time()
# Uncomment for fiat exchange rate -----
exchange_rate <- fromJSON('https://api.fixer.io/latest?base=USD')
EUR <- exchange_rate$rates$EUR
# Parallel process scraping with progress bar -----------------------------
cluster = makeCluster(cpucore, type = "SOCK")
registerDoSNOW(cluster)
pb <- txtProgressBar(max = length, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
attributes <- coins$slug
# Combine results and stop clusters ---------------------------------------
results = foreach(i = range, .options.snow = opts, .combine = rbind, .packages = "rvest") %dopar%
abstracts(attributes[i])
close(pb)
stopCluster(cluster)
# Cleanup results and fix names -------------------------------------------
coinnames <- data_frame(symbol = coins$symbol, name = coins$name, rank = coins$rank)
results <- merge(results, coinnames)
marketdata <- cleanUp(results)
write.csv(marketdata, file, row.names = FALSE)
print(proc.time() - ptm)
test <- fread("Crypto-Markets_2017-11-12.csv")
test <- data.table::fread("Crypto-Markets_2017-11-12.csv")
View(test)
View(results)
